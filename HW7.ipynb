{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Shiyue Cao\n",
    "\n",
    "USCID: 8583755038\n",
    "\n",
    "Email: shiyuec@usc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "1. Generative Models for Text\n",
    "\n",
    "# (a) In this problem, we are trying to build a generative model to mimic the writing style of prominent British Mathematician, Philosopher, prolific writer, and political activist, Bertrand Russell.\n",
    "\n",
    "# (b) Download the following books from Project Gutenberg http://www.gutenberg.org/ebooks/author/355 in text format\n",
    "\n",
    "# (c) LSTM: Train an LSTM to mimic Russell’s style and thoughts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Concatenate your text files to create a corpus of Russell’s writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "corpus = open(\"./src/corpus\", mode=\"a\")\n",
    "for filename in os.listdir(\"./data\"):\n",
    "    with open(\"./data/\"+filename, encoding='ascii', errors='ignore') as book:\n",
    "        for line in book:\n",
    "            corpus.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Use a character-level representation for this model by using extended ASCII that has N = 256 characters. Each character will be encoded into a an integer using its ASCII code. Rescale the integers to the range [0, 1], because LSTM uses a sigmoid activation function. LSTM will receive the rescaled integers as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_set = set(open('./src/corpus').read().lower())\n",
    "char_set = sorted(list(char_set))\n",
    "char_2_float = dict()\n",
    "char_2_int = dict()\n",
    "int_2_char = dict()\n",
    "i = 0\n",
    "for c in char_set:\n",
    "    char_2_float[c] = i/len(char_set)\n",
    "    i += 1\n",
    "i = 0\n",
    "for c in char_set:\n",
    "    char_2_int[c] = i\n",
    "    i += 1\n",
    "i = 0\n",
    "for c in char_2_float:\n",
    "    int_2_char[i] = c\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print(\"encode char to float in [0,1]\" )\n",
    "print(char_2_float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Choose a window size, e.g., W = 100\n",
    "## iv. Inputs to the network will be the first W −1 = 99 characters of each sequence, and the output of the network will be the Wth character of the sequence. Basically, we are training the network to predict each character using the 99 characters that precede it. Slide the window in strides of S = 1 on the text. For example, if W = 5 and S = 1 and we want to train the network with the sequence ABRACADABRA, The first input to the network will be ABRA and the corresponding output will be C. The second input will be BRAC and the second output will be A, etc.\n",
    "\n",
    "## v. Note that the output has to be encoded using a one-hot encoding scheme with N = 256 (or less) elements. This means that the network reads integers, but outputs a vector of N = 256 (or less) elements.¶\n",
    "\n",
    "## vi. Use a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "## vii. Use a Softmax output layer to yield a probability prediction for each of the characters between 0 and 1. This is actually a character classification problem with N classes. Choose log loss (cross entropy) as the objective function for the network (research what it means).\n",
    "## viii. We do not use a test dataset. We are using the whole training dataset to learn the probability of each character in a sequence. We are not seeking for a very accurate model. Instead we are interested in a generalization of the dataset that can mimic the gist of the text.\n",
    "\n",
    "## ix. Choose a reasonable number of epochs for training, considering your computational power (e.g., 30, although the network will need more epochs to yield a better model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Activation\n",
    "\n",
    "char_set = set(open('./src/corpus').read().lower())\n",
    "char_set = sorted(list(char_set))\n",
    "char_2_float = dict()\n",
    "char_2_int = dict()\n",
    "int_2_char = dict()\n",
    "i = 0\n",
    "for c in char_set:\n",
    "    char_2_float[c] = i/len(char_set)\n",
    "    i += 1\n",
    "i = 0\n",
    "for c in char_set:\n",
    "    char_2_int[c] = i\n",
    "    i += 1\n",
    "i = 0\n",
    "for c in char_2_float:\n",
    "    int_2_char[i] = c\n",
    "    i += 1\n",
    "\n",
    "\n",
    "chars = open('./src/corpus').read().lower()\n",
    "total_chars = len(chars)\n",
    "\n",
    "W = 99\n",
    "train_data = []\n",
    "train_target = []\n",
    "for i in range(0, total_chars - W):\n",
    "    input_char = chars[i:i + W]\n",
    "    output_char = chars[i + W]\n",
    "    p = []\n",
    "    for c in input_char:\n",
    "        p.append(char_2_float[c])\n",
    "    train_data.append(p)\n",
    "    train_target.append(char_2_int[output_char])\n",
    "\n",
    "train_data = np.reshape(train_data, (len(train_data), W, 1))\n",
    "train_target = np_utils.to_categorical(train_target)\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_target.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "model.add(Dense(train_target.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='./checkpoint/{epoch:02d}-{loss:.2f}.hdf5', monitor='loss',  save_best_only=True, mode='min', verbose=0)\n",
    "model.fit(train_data, train_target, batch_size=512,\n",
    "          epochs=30, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x. Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xi. Use the network with the best weights to generate 1000 characters, using the following text as initialization of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit1198b6c92e9e47f6a167d897e560d448"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}